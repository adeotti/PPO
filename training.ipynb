{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3530715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch,gym,random\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gym.wrappers import GrayScaleObservation,FrameStack\n",
    "from gym.vector import SyncVectorEnv\n",
    "import numpy as np\n",
    "    #\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #\n",
    "lr = 0.0001\n",
    "_lambda_ = 1.0\n",
    "gamma = 0.99\n",
    "epsilon = 0.2\n",
    "c1 = 0.5\n",
    "c2 = 0.01\n",
    "    # \n",
    "num_env = 8\n",
    "skip_frame = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6661adf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Wrapper): \n",
    "        def __init__(self,env,skip):\n",
    "            super().__init__(env)\n",
    "            self.skip = skip\n",
    "            self.score = 0\n",
    "            self.current_x_pos = 40\n",
    "            self.penalty = 0\n",
    "            \n",
    "        def step(self, action):\n",
    "            total_reward = 0  \n",
    "            for _ in range(self.skip):\n",
    "                obs,_,done,truncared,info = self.env.step(action)\n",
    "\n",
    "            if info.get(\"life\") == 2 : \n",
    "                if info.get(\"score\") > self.score :\n",
    "                    total_reward = ((info.get(\"score\") - self.score) / 10. ) + 10\n",
    "                    self.score = info.get(\"score\") \n",
    "                elif info.get(\"x_pos\") > self.current_x_pos :\n",
    "                    self.current_x_pos = info.get(\"x_pos\")\n",
    "                    total_reward = 15\n",
    "                    self.penalty = 0\n",
    "                else : \n",
    "                    if info.get(\"x_pos\") != 40 :\n",
    "                        self.penalty += -1\n",
    "                        total_reward += max(self.penalty,-10)\n",
    "            else:\n",
    "                total_reward = -20\n",
    "                done = True\n",
    "            return obs,total_reward,done,truncared,info\n",
    "\n",
    "        def reset(self, **kwargs):\n",
    "            self.score = 0\n",
    "            self.current_x_pos = 40\n",
    "            obs, info = self.env.reset()\n",
    "            return obs,info\n",
    "\n",
    "def make_env(): \n",
    "    x = gym_super_mario_bros.make('SuperMarioBros-v3',apply_api_compatibility=True) \n",
    "    x = CustomEnv(x,skip_frame) \n",
    "    x = JoypadSpace(x, SIMPLE_MOVEMENT)  \n",
    "    x = GrayScaleObservation(x,True)\n",
    "    x = SyncVectorEnv(env_fns=[lambda : x] * num_env)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1b00fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv2d(16,1,1,0)\n",
    "        self.conv2 = nn.LazyConv2d(16,2,2,1)\n",
    "        self.conv3 = nn.LazyConv2d(16,2,2,1)\n",
    "        self.conv4 = nn.LazyConv2d(16,2,2,1)\n",
    "        self.output1 = nn.LazyLinear(500)\n",
    "        self.output2 = nn.LazyLinear(80)\n",
    "\n",
    "        self.policy_head = nn.LazyLinear(7)\n",
    "        self.value_head = nn.LazyLinear(1)\n",
    "        self.optim = Adam(self.parameters(),lr=lr)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = torch.flatten(x,start_dim=1)  \n",
    "        x = F.relu(self.output1(x))\n",
    "        x = F.relu(self.output2(x))\n",
    "        policy_output = self.policy_head(x)\n",
    "        value_output = self.value_head(x)\n",
    "        return F.softmax(policy_output,-1),value_output\n",
    "      \n",
    "model = network().to(device)\n",
    "model.forward(torch.rand((8,3,240,256),dtype=torch.float32,device=device))\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d717bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc,sys\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self,env : SyncVectorEnv):\n",
    "        self.network = model\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self._lambda_ = _lambda_\n",
    "        self.data = []\n",
    "        self.pointer = 0\n",
    "        self._observation,info = self.env.reset()\n",
    "    \n",
    "    def rollout(self,batchsize):\n",
    "        self.clear()\n",
    "        with torch.no_grad(): \n",
    "            for n in range(batchsize):\n",
    "                self._observation = torch.from_numpy(self._observation.copy()).permute(0,-1,1,2).to(device,torch.float32) # [num_env, 3, 90, 90])\n",
    "                policy_output , value = self.network.forward(self._observation)\n",
    "                distribution = Categorical(policy_output)\n",
    "                action = distribution.sample()\n",
    "                prob = distribution.log_prob(action)\n",
    "                state,reward,done,_,_ = self.env.step(action.tolist())  \n",
    "                \n",
    "                if done.any(): \n",
    "                    done_idx = np.where(done == True)[0] \n",
    "                    for i in done_idx:\n",
    "                        state[i],_ = self.env.envs[i].reset()\n",
    "         \n",
    "                self.data.append([self._observation,\n",
    "                                  torch.tensor(reward,dtype=torch.float32),value,prob,action,done])\n",
    "                self._observation = state\n",
    " \n",
    "        _,rewards,values,_,_,_ = zip(*self.data) # compute advantages \n",
    "        _rewards = torch.stack(rewards).to(device) \n",
    "        _values = torch.stack(values).squeeze() \n",
    "\n",
    "        zeros = torch.zeros(1,_values.shape[-1],device=device,dtype=torch.float32)\n",
    "        _values = torch.cat([_values,zeros])\n",
    "        assert _rewards.dtype == _values.dtype == torch.float32\n",
    "\n",
    "        advantages = torch.zeros_like(_rewards,device=device,dtype=torch.float32)\n",
    "        gae = 0.0\n",
    "        for n in reversed(range(len(_rewards))):\n",
    "            td = _rewards[n] + self.gamma * _values[n+1] - _values[n] \n",
    "            gae = td + (self._lambda_ * self.gamma * gae) \n",
    "            advantages[n] = gae\n",
    "\n",
    "        for data,item in zip(self.data,advantages): # append advantages to data\n",
    "            data.append(item)\n",
    "        random.shuffle(self.data) \n",
    "\n",
    "        sys.exit(len(self.data))\n",
    "\n",
    "    def sample(self,number):\n",
    "        output = self.data[self.pointer:self.pointer+number]\n",
    "        self.pointer+=number\n",
    "        states,rewards,values,logProb,actions,done,advantages = zip(*output)\n",
    "        return states,actions,rewards,values,logProb,advantages,done\n",
    "\n",
    "    def clear(self):\n",
    "        self.data = []\n",
    "        self.pointer = 0\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c8163ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "class agent:\n",
    "    def __init__(self,):\n",
    "        self.env = make_env()\n",
    "        self.memory = Memory(self.env)\n",
    "        self.network = model\n",
    "        self.writter = SummaryWriter(\"./\")\n",
    "\n",
    "    def save(self,k):\n",
    "        checkpoint = {\n",
    "            \"model_state\" : self.network.state_dict(),\n",
    "            \"optim_state\" : self.network.optim.state_dict(),\n",
    "            \"lr\" : lr,\n",
    "            \"c2\" : c2,\n",
    "            \"c1\" : c1,\n",
    "            \"epsilon\" : epsilon\n",
    "        }\n",
    "        torch.save(checkpoint,f\"./mario{k}\")\n",
    "\n",
    "    def train(self,num_game,batchsize,minibatch,optim_step):\n",
    "        for traj in tqdm(range(num_game),total = num_game):\n",
    "            self.memory.rollout(batchsize)\n",
    "\n",
    "            for _ in range(batchsize//minibatch):\n",
    "                states,actions,rewards,values,old_log_prob,advantages,done = self.memory.sample(minibatch)    \n",
    "                _rewards = torch.mean(torch.stack(rewards)).round(decimals=4)\n",
    "                 \n",
    "                for _ in range(optim_step): \n",
    "                    _advantages = torch.stack(advantages).view(-1)\n",
    "                    _values = torch.stack(values).squeeze().view(-1)\n",
    "                    _old_log_prob = torch.stack(old_log_prob) \n",
    "                    \n",
    "                    vtarget = _advantages + _values\n",
    "                    norm_vtarget = (vtarget - vtarget.mean()) / (vtarget.std() + 1e-10)\n",
    "                    loss_critic = torch.mean(torch.pow((_values - norm_vtarget),2)) \n",
    "                \n",
    "                    stacked_states = torch.stack(states) # [minibatch, num_env, channel, image_shape]\n",
    "                    flat_states = torch.flatten(stacked_states,0,1).to(device) # [(minibatch * num_env), channel, image_shape]\n",
    "                    p_output,_ = self.network.forward(flat_states)\n",
    "                    policy_output = p_output.view(minibatch,num_env,7)\n",
    "                    dist = Categorical(policy_output)   \n",
    "                    stacked_actions = torch.stack(actions)  \n",
    "                    new_log_prob = dist.log_prob(stacked_actions)\n",
    "\n",
    "                    ratio = torch.exp(new_log_prob - _old_log_prob).view(-1)\n",
    "                    norm_advantage  = (_advantages - _advantages.mean()) / (_advantages.std() + 1e-10) \n",
    "                    proxy1 = ratio * norm_advantage\n",
    "                    proxy2 = torch.clamp(ratio ,1-epsilon,1+epsilon) * norm_advantage\n",
    "                    loss_policy = -torch.mean(torch.min(proxy1,proxy2))\n",
    "    \n",
    "                    _entropy = -(policy_output * torch.log(policy_output + 1e-10)).sum(dim=2)\n",
    "                    entropy = _entropy.sum(dim=1).mean()\n",
    "    \n",
    "                    total_loss = loss_policy + (c1*loss_critic) - (c2*entropy)\n",
    "    \n",
    "                    self.network.optim.zero_grad()\n",
    "                    total_loss.backward(retain_graph=True)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
    "                    self.network.optim.step()\n",
    "                    \n",
    "            self.writter.add_scalar(\"main/reward\",_rewards,traj)\n",
    "            self.writter.add_scalar(\"main/entropy\",entropy,traj)\n",
    "            self.writter.add_scalar(\"main/total loss\",total_loss,traj)\n",
    "            self.writter.add_scalar(\"main/loss policy\",loss_policy,traj)\n",
    "            self.writter.add_scalar(\"main/loss critic\",loss_critic,traj) \n",
    "        \n",
    "            if traj % 20 == 0 : # save every ...k steps\n",
    "                self.save(traj)\n",
    "                \n",
    "        self.save(\"_end\")\n",
    "\n",
    "test = agent()\n",
    "#test.train(num_game=5,batchsize=10,minibatch=5,optim_step=5)  # rollout test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2966e981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\14385\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "368\n",
      "58\n",
      "63\n",
      "399\n",
      "238\n",
      "133\n",
      "178\n",
      "69\n",
      "54\n",
      "217\n",
      "562\n",
      "460\n",
      "529\n",
      "73\n",
      "123\n",
      "69\n",
      "885\n"
     ]
    }
   ],
   "source": [
    "class test:\n",
    "    @staticmethod\n",
    "    def make_test_env(): \n",
    "        x = gym_super_mario_bros.make('SuperMarioBros-v3',apply_api_compatibility=True,render_mode=\"human\") \n",
    "        x = CustomEnv(x,skip_frame) \n",
    "        x = JoypadSpace(x, SIMPLE_MOVEMENT)  \n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def run(start,num_game):\n",
    "        if start:\n",
    "            with torch.no_grad():\n",
    "                model = network()\n",
    "                chk = torch.load(\"./mario1440\",map_location=\"cpu\")\n",
    "                model.load_state_dict(chk[\"model_state\"],strict=False)\n",
    "                env = __class__.make_test_env()\n",
    "                done = True\n",
    "                step=0\n",
    "                for _ in range(num_game):\n",
    "                    if done:\n",
    "                        state,_ = env.reset()\n",
    "                        print(step)\n",
    "                        step = 0\n",
    "                    state = torch.from_numpy(state.copy()).permute(-1,0,1).to(torch.float32).unsqueeze(0) \n",
    "                    dist,_ = model.forward(state)\n",
    "                    action = Categorical(dist).sample().item()\n",
    "                    state, reward, done, info,_ = env.step(action)\n",
    "                    step+=1\n",
    "                    env.render()\n",
    "\n",
    "                env.close()\n",
    "\n",
    "test.run(start=True,num_game=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
