{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e60e2f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install numpy==1.25.2\\n!pip install torch==2.7.0\\n!pip install torchvision==0.22.0\\n!pip install gym_super_mario_bros==7.4.0\\n!pip install gym==0.26.2\\n!pip install nes_py==8.2.1\\n!pip install gymnasium==0.29.1\\n!pip install opencv-python==4.11.0.86\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "!pip install numpy==1.25.2\n",
    "!pip install torch==2.7.0\n",
    "!pip install torchvision==0.22.0\n",
    "!pip install gym_super_mario_bros==7.4.0\n",
    "!pip install gym==0.26.2\n",
    "!pip install nes_py==8.2.1\n",
    "!pip install gymnasium==0.29.1\n",
    "!pip install opencv-python==4.11.0.86\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3530715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch,gym,random\n",
    "from dataclasses import dataclass\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gym.wrappers import GrayScaleObservation,FrameStack,ResizeObservation\n",
    "from gym.vector import AsyncVectorEnv\n",
    "import numpy as np\n",
    "\n",
    "@dataclass(frozen=False)\n",
    "class hypers:\n",
    "    seed : int = 42\n",
    "    lambda_ : float = 1.0\n",
    "    gamma : float = 0.99\n",
    "    epsilon : float = 0.2\n",
    "    lr : float = 1e-4\n",
    "    critic_coeff : float = 5e-1\n",
    "    policy_ceoff : int = 3e3\n",
    "    entropy_coeff : float = 1e-2\n",
    "    skip_frame : int = 4\n",
    "    num_stack : int  = 4\n",
    "    obs_shape : tuple[int,int] = (100,100) # observation shape\n",
    "    num_env : int = 5 # 30 \n",
    "    num_game : int = 2_400\n",
    "    batchsize : int = 100 # 512 \n",
    "    minibatch : int = 16\n",
    "    optim_steps : int = 10\n",
    "    # 30*1400*512 ~ 20 M steps, training target\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "\n",
    "configs = hypers()\n",
    "\n",
    "torch.manual_seed(configs.seed)\n",
    "random.seed(configs.seed)\n",
    "np.random.seed(configs.seed)\n",
    "torch.cuda.manual_seed(configs.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19fa5239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Wrapper): \n",
    "        def __init__(self,env,skip):\n",
    "            super().__init__(env)\n",
    "            self.skip = skip\n",
    "            \n",
    "        def step(self, action):\n",
    "            total_reward = 0  \n",
    "            for _ in range(self.skip):\n",
    "                obs,reward,done,truncared,info = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    self.reset()\n",
    "                    return obs,(total_reward/10.),done,truncared,info\n",
    "            return obs,(total_reward/10.),done,truncared,info\n",
    "\n",
    "        def reset(self, **kwargs):\n",
    "            obs,info = self.env.reset(**kwargs)\n",
    "            return obs,info\n",
    "\n",
    "def make_env():\n",
    "    def env():\n",
    "        x = gym_super_mario_bros.make(\"SuperMarioBros-v0\", apply_api_compatibility=True)\n",
    "        x = JoypadSpace(x, SIMPLE_MOVEMENT)\n",
    "        x = ResizeObservation(x, configs.obs_shape)\n",
    "        x = CustomEnv(x, configs.skip_frame)\n",
    "        x = GrayScaleObservation(x, keep_dim=True)\n",
    "        x = FrameStack(x, configs.num_stack) \n",
    "        return x\n",
    "    return AsyncVectorEnv([env for _ in range(configs.num_env)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "670aed79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Resume training\\nchk = torch.load(\"/kaggle/input/1_m190/pytorch/default/1/mario190\",map_location=configs.device)\\nmodel.load_state_dict(chk[\"model_state\"],strict=True)\\nmodel.module.optim.load_state_dict(chk[\"optim_state\"])\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv2d(32,1,1,0)\n",
    "        self.conv2 = nn.LazyConv2d(32,3,2,2)\n",
    "        self.conv3 = nn.LazyConv2d(32,3,2,2)\n",
    "        self.conv4 = nn.LazyConv2d(32,3,2,2)\n",
    "        self.output = nn.LazyLinear(80)\n",
    "\n",
    "        self.policy_head = nn.LazyLinear(7)\n",
    "        self.value_head = nn.LazyLinear(1)\n",
    "        self.optim = torch.optim.Adam(self.parameters(),lr=configs.lr)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = torch.flatten(x,start_dim=1) # -> torch.Size([32, 169])\n",
    "        x = F.relu(self.output(x))\n",
    "        policy_output = self.policy_head(x)\n",
    "        value_output = self.value_head(x)\n",
    "        return F.softmax(policy_output,-1),value_output\n",
    "\n",
    "def init_weights(layer):\n",
    "    if isinstance(layer,(nn.Conv2d,nn.Linear)):\n",
    "        nn.init.orthogonal_(layer.weight)\n",
    "        nn.init.constant_(layer.bias,0.0)\n",
    "     \n",
    "model = network().to(configs.device)\n",
    "model.forward(\n",
    "    torch.rand((configs.num_env,configs.num_stack,*(configs.obs_shape)),dtype=torch.float32,device=configs.device)\n",
    ")\n",
    "model.apply(init_weights)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.value_head.weight.data *= 100\n",
    "    model.value_head.bias.data.fill_(20.)\n",
    "    \n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "\"\"\"\n",
    "# Resume training\n",
    "chk = torch.load(\"/kaggle/input/1_m190/pytorch/default/1/mario190\",map_location=configs.device)\n",
    "model.load_state_dict(chk[\"model_state\"],strict=True)\n",
    "model.module.optim.load_state_dict(chk[\"optim_state\"])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "375af44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[  3.7680,   9.1783,   4.4187,  22.9356,   5.1605,  -6.7229,   9.5521,\n",
       "          10.7275,  -6.9100,  -2.8339, -13.7920,  -2.5826,   2.8348,   8.8232,\n",
       "          17.6081,  26.2936, -26.7314,  -4.0806,  -8.7103,  10.7763,   1.9723,\n",
       "          13.0598,   4.1047,  -3.6889, -15.5146,  14.4631, -21.3918, -17.3632,\n",
       "          -1.4463,  -5.0509, -18.0579, -21.6764,  -7.1621,  -4.0634,   7.3665,\n",
       "           5.7064, -17.4375,  -1.9364,   4.6773,  -7.1996,   9.9093,   7.9119,\n",
       "         -13.0704, -14.1722,   8.0836,   3.3512,   1.8710, -10.3647,  -9.3560,\n",
       "           0.2052,  -4.8783, -19.4190,  15.1575,  -6.1900,  -9.8598,   5.6194,\n",
       "           5.9801,  -4.8462,   2.7027, -12.1573, -17.6636,  12.9699,   3.7151,\n",
       "          -7.6713,   6.9944,  10.9111,   0.7166,   5.5400,   0.0663,  15.1733,\n",
       "          -0.7215,  -2.2825,  -0.1588,  10.3585,  26.1236,   4.8838, -18.1959,\n",
       "           5.8952,  -2.5848,  -6.8745]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module.value_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d717bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc,sys\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self,env : AsyncVectorEnv):\n",
    "        self.env = env\n",
    "        self.gamma = configs.gamma\n",
    "        self._lambda_ = configs.lambda_\n",
    "        self.data = []\n",
    "        self.pointer = 0\n",
    "        self.episode_reward = np.zeros(self.env.num_envs, dtype=np.float32)\n",
    "        self.finished_reward = deque(maxlen=5)\n",
    "        self.log_total_steps = deque(maxlen=5)\n",
    "        self.total_steps = torch.zeros(configs.num_env)\n",
    "    \n",
    "    def rollout(self,batchsize,network : network):\n",
    "        self.clear()\n",
    "        self._observation,_ = self.env.reset()\n",
    "        for n in range(batchsize):\n",
    "            self._observation = torch.from_numpy(np.array(self._observation)).squeeze(-1).to(configs.device,torch.float32) / 255.\n",
    "            with torch.no_grad():\n",
    "                policy_output, value = network.forward(self._observation)\n",
    "                distribution = Categorical(policy_output)\n",
    "                action = distribution.sample()\n",
    "                prob = distribution.log_prob(action)\n",
    "\n",
    "            state,reward,done,_,_ = self.env.step(action.tolist())\n",
    "\n",
    "            for i in range(self.env.num_envs): # reward per episode in a deque\n",
    "                self.episode_reward[i] += reward[i] \n",
    "                self.total_steps[i] += 1\n",
    "                if done[i]:\n",
    "                    self.finished_reward.append(self.episode_reward[i])\n",
    "                    self.log_total_steps.append(self.total_steps[i])\n",
    "                    self.episode_reward[i] = 0\n",
    "                    self.total_steps[i] = 0\n",
    "                    \n",
    "            self.data.append(\n",
    "                [\n",
    "                    self._observation,\n",
    "                    torch.tensor(reward,dtype=torch.float32),\n",
    "                    value,\n",
    "                    prob,\n",
    "                    action,\n",
    "                    done,\n",
    "                    distribution\n",
    "                ]\n",
    "            )\n",
    "            self._observation = state\n",
    "    \n",
    "        _,rewards,values,_,_,_,_ = zip(*self.data) # advantages \n",
    "        _rewards = torch.stack(rewards).to(configs.device) \n",
    "        values = torch.stack(values).squeeze(-1) \n",
    "        \n",
    "        next_state = torch.from_numpy(np.array(self._observation)).squeeze(-1).to(configs.device,torch.float32) / 255.\n",
    "        with torch.no_grad():\n",
    "            _,next_value = network.forward(next_state)\n",
    "        _values = torch.cat([values,next_value.permute(-1,0)])\n",
    "        assert _rewards.dtype == _values.dtype == torch.float32 \n",
    "        advantages = torch.zeros_like(_rewards,device=configs.device,dtype=torch.float32)\n",
    "        gae = 0.0\n",
    "        for n in reversed(range(len(_rewards))):\n",
    "            td = _rewards[n] + self.gamma * _values[n+1] - _values[n]\n",
    "            gae = td + (self._lambda_ * self.gamma * gae) \n",
    "            advantages[n] = gae\n",
    "        for data,item in zip(self.data,advantages): # append advantages to data\n",
    "            data.append(item)\n",
    "            \n",
    "        random.shuffle(self.data) \n",
    "\n",
    "    def sample(self,number):\n",
    "        output = self.data[self.pointer:self.pointer+number]\n",
    "        self.pointer+=number\n",
    "        states,rewards,values,logProb,actions,done,old_logits,advantages = zip(*output)\n",
    "        return states,actions,rewards,values,logProb,advantages,old_logits,done\n",
    "\n",
    "    def clear(self):\n",
    "        self.data = []\n",
    "        self.pointer = 0\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    def traj_reward(self):\n",
    "        return self.finished_reward,self.log_total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c8163ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "torch.Size([5, 4, 100, 100])",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m torch.Size([5, 4, 100, 100])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchviz import make_dot\n",
    "from torch.distributions.kl import kl_divergence\n",
    "\n",
    "class agent:\n",
    "    def __init__(self,):\n",
    "        self.env = make_env()\n",
    "        self.network = model\n",
    "        self.memory = Memory(self.env)\n",
    "        self.writter = SummaryWriter(\"./\")\n",
    "\n",
    "    def save(self,k):\n",
    "        checkpoint = {\n",
    "            \"model_state\" : self.network.state_dict(),\n",
    "            \"optim_state\" : self.network.module.optim.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint,f\"./mario{k}\")\n",
    "\n",
    "    def train(self,num_game,batchsize,minibatch,optim_steps,graph_render = False):\n",
    "        for traj in range(num_game):\n",
    "            self.memory.rollout(batchsize,self.network)\n",
    "\n",
    "            for _ in range(batchsize//minibatch):\n",
    "                states,actions,rewards,values,old_log_prob,advantages,old_logits,done= self.memory.sample(minibatch) \n",
    "                _advantages = torch.stack(advantages).view(-1)\n",
    "                _values = torch.stack(values).squeeze().view(-1)\n",
    "                vtarget = (_advantages + _values)\n",
    "                assert vtarget.requires_grad == _values.requires_grad == False\n",
    "                explained_variance = 1.0 - (torch.var(vtarget - _values) / torch.var(vtarget + 1e-10))\n",
    "                old_logits = [dis.probs for dis in old_logits]\n",
    "                old_logits = torch.stack(old_logits)\n",
    "                cat_old_logits = Categorical(probs=old_logits)\n",
    "\n",
    "                stacked_states = torch.stack(states) # - > [minibatch, num_env, channel, image_shape]\n",
    "                flat_states = torch.flatten(stacked_states,0,1).to(configs.device) # - > [(minibatch * num_env), channel, image_shape]\n",
    "                _old_log_prob = torch.stack(old_log_prob).detach()\n",
    " \n",
    "                for _ in range(optim_steps): \n",
    "                    p_output,new_values = self.network.forward(flat_states)\n",
    "                    policy_output = p_output.view(minibatch,configs.num_env,7) # 7 = action space for a single env\n",
    "                    dist = Categorical(probs=policy_output)  \n",
    "                    kl = kl_divergence(cat_old_logits,dist).mean() \n",
    "                    stacked_actions = torch.stack(actions)  \n",
    "                    new_log_prob = dist.log_prob(stacked_actions)\n",
    "                    ratio = torch.exp(new_log_prob - _old_log_prob).view(-1)\n",
    "                    #norm_advantage  = (_advantages - _advantages.mean()) / (_advantages.std() + 1e-10) \n",
    "                    prox1 = ratio * _advantages\n",
    "                    prox2 = torch.clamp(ratio ,1-configs.epsilon,1+configs.epsilon) * _advantages\n",
    "\n",
    "                    loss_policy = -torch.mean(torch.min(prox1,prox2))  \n",
    "                    loss_critic = F.smooth_l1_loss(new_values.squeeze(),vtarget) * configs.critic_coeff\n",
    "                    _entropy = -(policy_output * torch.log(policy_output + 1e-10)).sum(dim=2)\n",
    "                    entropy = _entropy.sum(dim=1).mean() * configs.entropy_coeff\n",
    "\n",
    "                    total_loss = loss_policy + loss_critic - entropy \n",
    "\n",
    "                    if graph_render: # computation graph for debugging purposes\n",
    "                        make_dot(total_loss,dict(self.network.named_parameters())).render(\"Graph\",format=\"png\")\n",
    "                        graph_render = False\n",
    "                    \n",
    "                    self.network.module.optim.zero_grad()\n",
    "                    total_loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
    "                    self.network.module.optim.step()\n",
    "                    \n",
    "            self.writter.add_scalar(\"Policy/entropy\",entropy,traj)\n",
    "            self.writter.add_scalar(\"Policy/loss policy\",loss_policy,traj)\n",
    "            self.writter.add_scalar(\"Policy/KL\",kl,traj)\n",
    "            self.writter.add_scalar(\"Value/values\",_values.mean(),traj)\n",
    "            self.writter.add_scalar(\"Value/vtarget\",vtarget.mean(),traj)\n",
    "            self.writter.add_scalar(\"Value/value loss\",loss_critic,traj) \n",
    "            self.writter.add_scalar(\"Value/Explained variance\",explained_variance,traj)\n",
    "            self.writter.add_scalar(\"main/total loss\",total_loss,traj)\n",
    "            self.writter.add_scalar(\"main/epi rewards\",torch.tensor([self.memory.traj_reward()[0]]).mean(),traj) \n",
    "            self.writter.add_scalar(\"main/total steps\",torch.tensor([self.memory.traj_reward()[1]]).mean(),traj)\n",
    "\n",
    "            epireward = round(torch.tensor([self.memory.traj_reward()[0]]).mean().tolist(),2)\n",
    "            print(\n",
    "                f\"{traj}/1k | REWA {epireward} | ENTR {entropy:.2f} | POLI {loss_policy:.2f} | CRIT {loss_critic:.2f} | TLoss {total_loss:.2f} | Val {_values.mean():.2f} | Vtarg {vtarget.mean():.2f} | KL {kl:.4f}\"\n",
    "                )\n",
    "            if traj % 10 == 0 : # save every ~150k steps\n",
    "                self.save(traj)\n",
    "        \n",
    "        self.save(\"_end\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def run_training(run=False):\n",
    "        if run:\n",
    "            agnt = __class__()\n",
    "            agnt.train(\n",
    "                num_game=configs.num_game,\n",
    "                batchsize=configs.batchsize,\n",
    "                minibatch=configs.minibatch,\n",
    "                optim_steps=configs.optim_steps\n",
    "            ) \n",
    "            \n",
    "agent.run_training(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
