{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4642e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gymnasium==0.29.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8849e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch,sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
    "\n",
    "@dataclass\n",
    "class Hypers:\n",
    "    env = gym.make_vec(\n",
    "        \"CartPole-v1\",num_envs=10,vectorization_mode=\"sync\"\n",
    "    )\n",
    "    device = torch.device(\"cpu\")\n",
    "    lr = 1e-4\n",
    "    gamma = 0.99\n",
    "    lambda_ = 1.0\n",
    "    epsilon = 0.2\n",
    "\n",
    "    entropy_coeff = 1e-1\n",
    "    critic_coeff = 5e-1\n",
    " \n",
    "    total_games = 600\n",
    "    batchsize = 256\n",
    "    minibatch = 64\n",
    "    optim_steps = 6\n",
    "\n",
    "hypers = Hypers()\n",
    "\n",
    "class network(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.l1 = nn.LazyLinear(32)\n",
    "        self.l2 = nn.LazyLinear(32)\n",
    "        self.l3 = nn.LazyLinear(32)\n",
    "        self.policy = nn.LazyLinear(2)\n",
    "        self.value = nn.LazyLinear(1)\n",
    "        self.optim =  torch.optim.Adam(self.parameters(),lr=hypers.lr)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        policy = self.policy(x)\n",
    "        value = self.value(x)        \n",
    "        return F.softmax(policy,-1),value\n",
    "    \n",
    "def init_w(w):\n",
    "    if isinstance(w,nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(w.weight)\n",
    "        torch.nn.init.constant_(w.bias,0.0)\n",
    "\n",
    "model = network().to(hypers.device)\n",
    "model(torch.rand((5,4),dtype=torch.float32,device=hypers.device))\n",
    "model.apply(init_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f34b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "class replay_buffer:\n",
    "    def __init__(self,env : Hypers,model : network):\n",
    "        self.data = []\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.pointer = 0\n",
    "        self.reward = np.zeros(self.env.num_envs, dtype=np.float32)\n",
    "        self.reward_data = deque(maxlen=25)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def rollout(self,batchsize):\n",
    "        self.clear()\n",
    "        obs,_ = self.env.reset()\n",
    "        for n in range(batchsize):\n",
    "            obs = torch.from_numpy(obs).to(torch.float32)\n",
    "            probs,value = self.model(obs)\n",
    "            action_dist = Categorical(probs=probs)\n",
    "            action = action_dist.sample()\n",
    "            old_porbs = action_dist.log_prob(action)\n",
    "\n",
    "            new_state,reward,done,_,_ = self.env.step(action.tolist())\n",
    "            for n in range(self.env.num_envs): # reset done env\n",
    "                self.reward[n]+=reward[n]\n",
    "                if done[n]:\n",
    "                    self.reward_data.append(self.reward[n])\n",
    "                    self.reward[n] = 0\n",
    "       \n",
    "            self.data.append([\n",
    "                obs,\n",
    "                torch.from_numpy(reward).to(torch.float32),\n",
    "                value,\n",
    "                action,\n",
    "                old_porbs,\n",
    "                torch.from_numpy(done).to(torch.float32)]\n",
    "            )\n",
    "            obs = new_state\n",
    "    \n",
    "        next_state = torch.from_numpy(obs).to(torch.float32)\n",
    "        _,_next_value = self.model(next_state)\n",
    "        _,rewards,values,_,_,dones = zip(*self.data)\n",
    "        dones = torch.stack(dones)\n",
    "        rewards = torch.stack(rewards).to(torch.float32)\n",
    "        _values = torch.stack((values)).squeeze()\n",
    "        values = torch.cat((_values,_next_value.permute(-1,0)))\n",
    "        advantage = torch.zeros_like(rewards,dtype=torch.float32)\n",
    "        gae = 0.0\n",
    "        for n in reversed(range(len(rewards))):\n",
    "            td = rewards[n] + (hypers.gamma * values[n+1] * (1-dones[n])) - values[n]\n",
    "            gae = td + (hypers.lambda_ * hypers.gamma * gae * (1-dones[n]))\n",
    "            advantage[n] = gae\n",
    "\n",
    "        for data,item in zip(self.data,advantage):\n",
    "            data.append(item)\n",
    "    \n",
    "    def sample(self,minibatch):\n",
    "        output = self.data[self.pointer:self.pointer+minibatch]\n",
    "        if len(output)== 0 : raise ValueError(\"replay buffer is empty\")\n",
    "        self.pointer+=minibatch\n",
    "        Stack_ = lambda x : torch.stack(x).squeeze()\n",
    "        states,_,values,actions,old_probs,_,advantages = zip(*output)\n",
    "        return Stack_(states),Stack_(values) ,Stack_(actions),Stack_(old_probs),Stack_(advantages)\n",
    "         \n",
    "    def clear(self):\n",
    "        self.pointer = 0\n",
    "        self.data = []\n",
    "\n",
    "    def reward_fn(self):\n",
    "        return torch.tensor(self.reward_data).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7db24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self):\n",
    "        self.model = model\n",
    "        self.env = hypers.env\n",
    "        self.memory = replay_buffer(self.env,self.model)\n",
    "    \n",
    "    def save(self):\n",
    "        chk = {\n",
    "            \"model_state\" : self.model.state_dict(),\n",
    "            \"optim_state\" : self.model.optim.state_dict()\n",
    "        }\n",
    "        torch.save(chk,\"./CartPole.pth\")\n",
    "\n",
    "    def run(self,total_games,batchsize,minibatch,optim_step):\n",
    "        for traj in tqdm(range(total_games),total=total_games):\n",
    "            self.memory.rollout(batchsize)\n",
    "            for _ in range(batchsize//minibatch):\n",
    "                _states,_values,_actions,_oldprobs,_advantages = self.memory.sample(minibatch)\n",
    "                vtarget = _advantages + _values\n",
    "                for _ in range(optim_step):\n",
    "                    probs,values = self.model(_states)\n",
    "                    dist = Categorical(probs)\n",
    "                    new_probs = dist.log_prob(_actions)\n",
    "                    ratio = torch.exp(new_probs - _oldprobs)\n",
    "                    surr1 = ratio *_advantages\n",
    "                    surr2 = torch.clamp(ratio,1-hypers.epsilon,1+hypers.epsilon) * _advantages\n",
    "                    loss_policy = -torch.mean(torch.min(surr1,surr2))\n",
    "                    entropy = dist.entropy().mean() * hypers.entropy_coeff\n",
    "                    loss_critic = F.mse_loss(values.squeeze(-1),vtarget) * hypers.critic_coeff\n",
    "                    total_loss = loss_policy + loss_critic - entropy\n",
    "                    self.model.optim.zero_grad()\n",
    "                    total_loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(self.model.parameters(),0.5)\n",
    "                    self.model.optim.step()\n",
    "\n",
    "            if traj % 25 == 0:\n",
    "                print(self.memory.reward_fn())\n",
    "                self.save()\n",
    "            \n",
    "t = PPO()\n",
    "t.run(hypers.total_games,hypers.batchsize,hypers.minibatch,hypers.optim_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
