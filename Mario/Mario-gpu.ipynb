{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":455622,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":369482,"modelId":390353},{"sourceId":456448,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":370141,"modelId":391029},{"sourceId":456636,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":370295,"modelId":391180},{"sourceId":456713,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":370342,"modelId":391230},{"sourceId":456822,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":370436,"modelId":391336},{"sourceId":457380,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":370890,"modelId":391789}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install numpy==1.25.2\n!pip install torch==2.7.0\n!pip install torchvision==0.22.0\n!pip install gym_super_mario_bros==7.4.0\n!pip install gym==0.26.2\n!pip install nes_py==8.2.1\n!pip install gymnasium==0.29.1\n!pip install opencv-python==4.11.0.86","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T23:16:38.068087Z","iopub.execute_input":"2025-06-28T23:16:38.068451Z","iopub.status.idle":"2025-06-28T23:21:19.901095Z","shell.execute_reply.started":"2025-06-28T23:16:38.068423Z","shell.execute_reply":"2025-06-28T23:21:19.900379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport torch,gym,random\nfrom dataclasses import dataclass\nimport gym_super_mario_bros\nfrom nes_py.wrappers import JoypadSpace\nfrom gym_super_mario_bros.actions import SIMPLE_MOVEMENT\nfrom gym.wrappers import GrayScaleObservation,FrameStack,ResizeObservation\nfrom gym.vector import SyncVectorEnv,AsyncVectorEnv\nimport numpy as np\n\n@dataclass(frozen=False)\nclass hypers:\n    seed : int = 42\n    lambda_ : float = 1.0\n    gamma : float = 0.99\n    epsilon : float = 0.2\n    lr : float = 1e-4\n    critic_coeff : float = 5e-1\n    policy_ceoff : int = 3e3\n    entropy_coeff : float = 1e-1\n    skip_frame : int = 4\n    num_stack : int  = 4\n    obs_shape : tuple[int,int] = (100,100) # observation shape\n    num_env : int = 30 \n    num_game : int = 1_400\n    batchsize : int = 512 \n    minibatch : int = 16\n    optim_steps : int = 10\n    # 30*1400*512 ~ 20 M steps, training target\n    device = torch.device(\n        \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    )\n\nconfigs = hypers()\n\ntorch.manual_seed(configs.seed)\nrandom.seed(configs.seed)\nnp.random.seed(configs.seed)\ntorch.cuda.manual_seed(configs.seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T23:21:19.903027Z","iopub.execute_input":"2025-06-28T23:21:19.903329Z","iopub.status.idle":"2025-06-28T23:21:22.367513Z","shell.execute_reply.started":"2025-06-28T23:21:19.903292Z","shell.execute_reply":"2025-06-28T23:21:22.366954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomEnv(gym.Wrapper): \n        def __init__(self,env,skip):\n            super().__init__(env)\n            self.skip = skip\n            self.score = 0\n            \n        def step(self, action):\n            total_reward = 0  \n            for _ in range(self.skip):\n                obs,reward,done,truncared,info = self.env.step(action)\n                total_reward += reward\n                if done:\n                    self.reset()\n                    return obs,(total_reward/10.),done,truncared,info\n            return obs,(total_reward/10.),done,truncared,info\n\n        def reset(self, **kwargs):\n            self.score = 0\n            obs,info = self.env.reset(**kwargs)\n            return obs,info\n\ndef make_env():\n    def env():\n        x = gym_super_mario_bros.make(\"SuperMarioBros-v0\", apply_api_compatibility=True)\n        x = JoypadSpace(x, SIMPLE_MOVEMENT)\n        x = ResizeObservation(x, configs.obs_shape)\n        x = CustomEnv(x, configs.skip_frame)\n        x = GrayScaleObservation(x, keep_dim=True)\n        x = FrameStack(x, configs.num_stack) \n        return x\n    return AsyncVectorEnv([env for _ in range(configs.num_env)])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T23:21:22.368152Z","iopub.execute_input":"2025-06-28T23:21:22.368469Z","iopub.status.idle":"2025-06-28T23:21:22.374796Z","shell.execute_reply.started":"2025-06-28T23:21:22.368450Z","shell.execute_reply":"2025-06-28T23:21:22.374275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass network(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.LazyConv2d(32,1,1,0)\n        self.conv2 = nn.LazyConv2d(32,3,2,2)\n        self.conv3 = nn.LazyConv2d(32,3,2,2)\n        self.conv4 = nn.LazyConv2d(32,3,2,2)\n        self.output = nn.LazyLinear(80)\n\n        self.policy_head = nn.LazyLinear(7)\n        self.value_head = nn.LazyLinear(1)\n        self.optim = torch.optim.Adam(self.parameters(),lr=configs.lr)\n        \n    def forward(self,x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        x = torch.flatten(x,start_dim=1) # -> torch.Size([32, 169])\n        x = F.relu(self.output(x))\n        policy_output = self.policy_head(x)\n        value_output = self.value_head(x)\n        return F.softmax(policy_output,-1),value_output\n\ndef init_weights(layer):\n    if isinstance(layer,(nn.Conv2d,nn.Linear)):\n        nn.init.orthogonal_(layer.weight)\n        nn.init.constant_(layer.bias,0.0)\n     \nmodel = network().to(configs.device)\nmodel.forward(\n    torch.rand((configs.num_env,configs.num_stack,*(configs.obs_shape)),dtype=torch.float32,device=configs.device)\n)\nmodel.apply(init_weights)\n\nwith torch.no_grad():\n    model.value_head.weight.data *= 100\n    model.value_head.bias.data.fill_(20.)\n    \nmodel = nn.DataParallel(model)\n\n#chk = torch.load(\"/kaggle/input/10_m330/pytorch/default/1/mario330\",map_location=configs.device)\n#model.load_state_dict(chk[\"model_state\"],strict=True)\n#model.module.optim.load_state_dict(chk[\"optim_state\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T23:21:22.375480Z","iopub.execute_input":"2025-06-28T23:21:22.375758Z","iopub.status.idle":"2025-06-28T23:21:24.230911Z","shell.execute_reply.started":"2025-06-28T23:21:22.375732Z","shell.execute_reply":"2025-06-28T23:21:24.230298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.module.value_head.weight","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T23:21:24.232892Z","iopub.execute_input":"2025-06-28T23:21:24.233525Z","iopub.status.idle":"2025-06-28T23:21:24.390156Z","shell.execute_reply.started":"2025-06-28T23:21:24.233503Z","shell.execute_reply":"2025-06-28T23:21:24.389583Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc,sys\nfrom torch.distributions import Categorical\nfrom collections import deque\n\n\nclass Memory:\n    def __init__(self,env : AsyncVectorEnv):\n        self.env = env\n        self.gamma = configs.gamma\n        self._lambda_ = configs.lambda_\n        self.data = []\n        self.pointer = 0\n        self.episode_reward = np.zeros(self.env.num_envs, dtype=np.float32)\n        self.finished_reward = deque(maxlen=5)\n        self.log_total_steps = deque(maxlen=5)\n        self.total_steps = torch.zeros(configs.num_env)\n    \n    def rollout(self,batchsize,network : network):\n        self.clear()\n        self._observation,_ = self.env.reset()\n        for n in range(batchsize):\n            self._observation = torch.from_numpy(np.array(self._observation)).squeeze(-1).to(configs.device,torch.float32) / 255.\n            with torch.no_grad():\n                policy_output, value = network.forward(self._observation)\n                distribution = Categorical(policy_output)\n                action = distribution.sample()\n                prob = distribution.log_prob(action)\n\n            state,reward,done,_,_ = self.env.step(action.tolist())\n\n            for i in range(self.env.num_envs): # reward per episode in a deque\n                self.episode_reward[i] += reward[i] \n                self.total_steps[i] += 1\n                if done[i]:\n                    self.finished_reward.append(self.episode_reward[i])\n                    self.log_total_steps.append(self.total_steps[i])\n                    self.episode_reward[i] = 0\n                    self.total_steps[i] = 0\n                    \n            self.data.append(\n                [\n                    self._observation,\n                    torch.tensor(reward,dtype=torch.float32),\n                    value,\n                    prob,\n                    action,\n                    done,\n                    distribution\n                ]\n            )\n            self._observation = state\n    \n        _,rewards,values,_,_,_,_ = zip(*self.data) # advantages \n        _rewards = torch.stack(rewards).to(configs.device) \n        values = torch.stack(values).squeeze(-1) \n        \n        next_state = torch.from_numpy(np.array(self._observation)).squeeze(-1).to(configs.device,torch.float32) / 255.\n        with torch.no_grad():\n            _,next_value = network.forward(next_state)\n        #zeros = torch.zeros(1,values.shape[-1],device=configs.device,dtype=torch.float32)\n        _values = torch.cat([values,next_value.permute(-1,0)])\n        assert _rewards.dtype == _values.dtype == torch.float32 \n        advantages = torch.zeros_like(_rewards,device=configs.device,dtype=torch.float32)\n        gae = 0.0\n        for n in reversed(range(len(_rewards))):\n            td = _rewards[n] + self.gamma * _values[n+1] - _values[n]\n            gae = td + (self._lambda_ * self.gamma * gae) \n            advantages[n] = gae\n        for data,item in zip(self.data,advantages): # append advantages to data\n            data.append(item)\n            \n        random.shuffle(self.data) \n\n    def sample(self,number):\n        output = self.data[self.pointer:self.pointer+number]\n        self.pointer+=number\n        states,rewards,values,logProb,actions,done,old_logits,advantages = zip(*output)\n        return states,actions,rewards,values,logProb,advantages,old_logits,done\n\n    def clear(self):\n        self.data = []\n        self.pointer = 0\n        gc.collect()\n        torch.cuda.empty_cache()\n    \n    def traj_reward(self):\n        return self.finished_reward,self.log_total_steps","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T23:26:01.722905Z","iopub.execute_input":"2025-06-28T23:26:01.723536Z","iopub.status.idle":"2025-06-28T23:26:19.438969Z","shell.execute_reply.started":"2025-06-28T23:26:01.723509Z","shell.execute_reply":"2025-06-28T23:26:19.438212Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.tensorboard import SummaryWriter\nfrom torch.distributions.kl import kl_divergence\n\nclass agent:\n    def __init__(self,):\n        self.env = make_env()\n        self.network = model\n        self.memory = Memory(self.env)\n        self.writter = SummaryWriter(\"./\")\n\n    def save(self,k):\n        checkpoint = {\n            \"model_state\" : self.network.state_dict(),\n            \"optim_state\" : self.network.module.optim.state_dict(),\n        }\n        torch.save(checkpoint,f\"./mario{k}\")\n\n    def train(self,num_game,batchsize,minibatch,optim_steps):\n        for traj in range(num_game):\n            self.memory.rollout(batchsize,self.network)\n\n            for _ in range(batchsize//minibatch):\n                states,actions,rewards,values,old_log_prob,advantages,old_logits,done= self.memory.sample(minibatch) \n                _advantages = torch.stack(advantages).view(-1)\n                _values = torch.stack(values).squeeze().view(-1)\n                vtarget = (_advantages + _values)\n                assert vtarget.requires_grad == _values.requires_grad == False\n                explained_variance = 1.0 - (torch.var(vtarget - _values) / torch.var(vtarget + 1e-10))\n                old_logits = [dis.probs for dis in old_logits]\n                old_logits = torch.stack(old_logits)\n                cat_old_logits = Categorical(probs=old_logits)\n\n                stacked_states = torch.stack(states) # - > [minibatch, num_env, channel, image_shape]\n                flat_states = torch.flatten(stacked_states,0,1).to(configs.device) # - > [(minibatch * num_env), channel, image_shape]\n                _old_log_prob = torch.stack(old_log_prob).detach()\n \n                for _ in range(optim_steps): \n                    p_output,new_values = self.network.forward(flat_states)\n                    policy_output = p_output.view(minibatch,configs.num_env,7) # 7 = action space for a single env\n                    dist = Categorical(probs=policy_output)  \n                    kl = kl_divergence(cat_old_logits,dist).mean() \n                    stacked_actions = torch.stack(actions)  \n                    new_log_prob = dist.log_prob(stacked_actions)\n                    ratio = torch.exp(new_log_prob - _old_log_prob).view(-1)\n                    #norm_advantage  = (_advantages - _advantages.mean()) / (_advantages.std() + 1e-10) \n                    prox1 = ratio * _advantages\n                    prox2 = torch.clamp(ratio ,1-configs.epsilon,1+configs.epsilon) * _advantages\n\n                    loss_policy = -torch.mean(torch.min(prox1,prox2))  \n                    loss_critic = F.smooth_l1_loss(new_values.squeeze(),vtarget) * configs.critic_coeff\n                    #_entropy = -(policy_output * torch.log(policy_output + 1e-10)).sum(dim=2)\n                    #entropy = _entropy.sum(dim=1).mean() * configs.entropy_coeff\n                    entropy = dist.entropy().mean() * configs.entropy_coeff\n\n                    total_loss = loss_policy + loss_critic - entropy \n\n                    self.network.module.optim.zero_grad()\n                    total_loss.backward()\n                    nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n                    self.network.module.optim.step()\n                    \n            self.writter.add_scalar(\"Policy/entropy\",dist.entropy().mean(),traj)\n            self.writter.add_scalar(\"Policy/loss policy\",loss_policy,traj)\n            self.writter.add_scalar(\"Policy/KL\",kl,traj)\n            self.writter.add_scalar(\"Value/values\",_values.mean(),traj)\n            self.writter.add_scalar(\"Value/vtarget\",vtarget.mean(),traj)\n            self.writter.add_scalar(\"Value/value loss\",loss_critic,traj) \n            self.writter.add_scalar(\"Value/Explained variance\",explained_variance,traj)\n            self.writter.add_scalar(\"main/total loss\",total_loss,traj)\n            self.writter.add_scalar(\"main/epi rewards\",torch.tensor([self.memory.traj_reward()[0]]).mean(),traj) \n            self.writter.add_scalar(\"main/total steps\",torch.tensor([self.memory.traj_reward()[1]]).mean(),traj)\n\n            epireward = round(torch.tensor([self.memory.traj_reward()[0]]).mean().tolist(),2)\n            print(\n                f\"{traj}/1k | REWA {epireward} | ENTR {entropy:.4f} | POLI {loss_policy:.2f} | CRIT {loss_critic:.2f} | TLoss {total_loss:.2f} | Val {_values.mean():.2f} | Vtarg {vtarget.mean():.2f} | KL {kl:.4f}\"\n                )\n            if traj % 10 == 0 : # save every ...k steps\n                self.save(traj)\n        \n        self.save(\"_end\")\n    \n    @staticmethod\n    def run_training(run=False):\n        if run:\n            agnt = __class__()\n            agnt.train(\n                num_game=configs.num_game,\n                batchsize=configs.batchsize,\n                minibatch=configs.minibatch,\n                optim_steps=configs.optim_steps\n            ) \n            \nagent.run_training(True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T23:21:43.356309Z","iopub.status.idle":"2025-06-28T23:21:43.356551Z","shell.execute_reply.started":"2025-06-28T23:21:43.356441Z","shell.execute_reply":"2025-06-28T23:21:43.356453Z"}},"outputs":[],"execution_count":null}]}