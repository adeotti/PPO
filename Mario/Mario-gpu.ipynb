{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T23:16:38.068451Z",
     "iopub.status.busy": "2025-06-28T23:16:38.068087Z",
     "iopub.status.idle": "2025-06-28T23:21:19.901095Z",
     "shell.execute_reply": "2025-06-28T23:21:19.900379Z",
     "shell.execute_reply.started": "2025-06-28T23:16:38.068423Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install gym_super_mario_bros==7.4.0\n",
    "!pip install gym==0.26.2\n",
    "!pip install nes_py==8.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T23:21:19.903329Z",
     "iopub.status.busy": "2025-06-28T23:21:19.903027Z",
     "iopub.status.idle": "2025-06-28T23:21:22.367513Z",
     "shell.execute_reply": "2025-06-28T23:21:22.366954Z",
     "shell.execute_reply.started": "2025-06-28T23:21:19.903292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch,gym\n",
    "from dataclasses import dataclass\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gym.wrappers import GrayScaleObservation,FrameStack,ResizeObservation\n",
    "from gym.vector import AsyncVectorEnv\n",
    "import numpy as np\n",
    "\n",
    "@dataclass(frozen=False)\n",
    "class hypers:\n",
    "    seed : int = 42\n",
    "    lambda_ : float = .99\n",
    "    gamma : float = .99\n",
    "    epsilon : float = .2\n",
    "    lr : float = 1e-4\n",
    "    critic_coeff : float = 5e-1\n",
    "    policy_ceoff : int = 3e3\n",
    "    entropy_coeff : float = 1e-1\n",
    "    skip_frame : int = 4\n",
    "    num_stack : int  = 4\n",
    "    obs_shape : tuple[int,int] = (100,100) # observation shape\n",
    "    num_env : int = 30 \n",
    "    num_game : int = 1_000\n",
    "    batchsize : int = 512 \n",
    "    minibatch : int = 16\n",
    "    optim_steps : int = 10\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "\n",
    "configs = hypers()\n",
    "\n",
    "torch.manual_seed(configs.seed)\n",
    "np.random.seed(configs.seed)\n",
    "torch.cuda.manual_seed(configs.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T23:21:22.368469Z",
     "iopub.status.busy": "2025-06-28T23:21:22.368152Z",
     "iopub.status.idle": "2025-06-28T23:21:22.374796Z",
     "shell.execute_reply": "2025-06-28T23:21:22.374275Z",
     "shell.execute_reply.started": "2025-06-28T23:21:22.368450Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Wrapper): \n",
    "        def __init__(self,env,skip):\n",
    "            super().__init__(env)\n",
    "            self.skip = skip\n",
    "            self.score = 0\n",
    "            \n",
    "        def step(self, action):\n",
    "            total_reward = 0  \n",
    "            for _ in range(self.skip):\n",
    "                obs,reward,done,truncared,info = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    obs,info = self.reset()\n",
    "                    return obs,(total_reward/10.),done,truncared,info\n",
    "            return obs,(total_reward/10.),done,truncared,info\n",
    "\n",
    "        def reset(self, **kwargs):\n",
    "            self.score = 0\n",
    "            obs,info = self.env.reset(**kwargs)\n",
    "            return obs,info\n",
    "\n",
    "def make_env():\n",
    "    def env():\n",
    "        x = gym_super_mario_bros.make(\"SuperMarioBros-v0\", apply_api_compatibility=True)\n",
    "        x = JoypadSpace(x, SIMPLE_MOVEMENT)\n",
    "        x = ResizeObservation(x, configs.obs_shape)\n",
    "        x = CustomEnv(x, configs.skip_frame)\n",
    "        x = GrayScaleObservation(x, keep_dim=True)\n",
    "        x = FrameStack(x, configs.num_stack) \n",
    "        return x\n",
    "    return AsyncVectorEnv([env for _ in range(configs.num_env)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T23:21:22.375758Z",
     "iopub.status.busy": "2025-06-28T23:21:22.375480Z",
     "iopub.status.idle": "2025-06-28T23:21:24.230911Z",
     "shell.execute_reply": "2025-06-28T23:21:24.230298Z",
     "shell.execute_reply.started": "2025-06-28T23:21:22.375732Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv2d(32,1,1,0)\n",
    "        self.conv2 = nn.LazyConv2d(32,3,2,2)\n",
    "        self.conv3 = nn.LazyConv2d(32,3,2,2)\n",
    "        self.conv4 = nn.LazyConv2d(32,3,2,2)\n",
    "        self.output = nn.LazyLinear(512)\n",
    "        self.policy_head = nn.LazyLinear(7)\n",
    "        self.value_head = nn.LazyLinear(1)\n",
    "        self.optim = torch.optim.Adam(self.parameters(),lr=configs.lr)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x) \n",
    "        x = torch.flatten(x,start_dim=1)  \n",
    "        x = F.relu(self.output(x))\n",
    "        policy_output = self.policy_head(x)\n",
    "        value_output = self.value_head(x)\n",
    "        return F.softmax(policy_output,-1),value_output\n",
    "\n",
    "def init_weights(layer):\n",
    "    if isinstance(layer,(nn.Conv2d,nn.Linear)):\n",
    "        nn.init.orthogonal_(layer.weight)\n",
    "        nn.init.constant_(layer.bias,0.0)\n",
    "     \n",
    "model = network().to(configs.device)\n",
    "model.forward(\n",
    "    torch.rand(\n",
    "        (1,configs.num_stack,*(configs.obs_shape)),dtype=torch.float32,device=configs.device\n",
    "    )\n",
    ")\n",
    "model.apply(init_weights)\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "#chk = torch.load(\"/kaggle/input/1_m350/pytorch/default/1/mario350\",map_location=configs.device)\n",
    "#model.load_state_dict(chk[\"model_state\"],strict=True)\n",
    "#model.module.optim.load_state_dict(chk[\"optim_state\"])\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt # last conv output visualization\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.LazyConv2d(32,1,1,0),nn.LazyConv2d(32,3,2,2),nn.LazyConv2d(32,3,2,2),nn.LazyConv2d(32,3,2,2)\n",
    ")\n",
    "net(torch.rand(1,4,100,100).to(torch.float32))\n",
    "\n",
    "env = make_env()\n",
    "obs,inf = env.reset()\n",
    "obs = torch.from_numpy(obs).squeeze().to(torch.float32)\n",
    "output = net(obs)\n",
    "fig,ax = plt.subplots(1,1,figsize=(4,4))\n",
    "ax.imshow(output[0].detach().numpy()), plt.axis(\"off\")\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T23:26:01.723536Z",
     "iopub.status.busy": "2025-06-28T23:26:01.722905Z",
     "iopub.status.idle": "2025-06-28T23:26:19.438969Z",
     "shell.execute_reply": "2025-06-28T23:26:19.438212Z",
     "shell.execute_reply.started": "2025-06-28T23:26:01.723509Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self,env : AsyncVectorEnv):\n",
    "        self.env = env\n",
    "        self.gamma = configs.gamma\n",
    "        self._lambda_ = configs.lambda_\n",
    "        self.data = []\n",
    "        self.pointer = 0\n",
    "        self.episode_reward = np.zeros(self.env.num_envs, dtype=np.float32)\n",
    "        self.finished_reward = deque(maxlen=30)\n",
    "        self.log_total_steps = deque(maxlen=30)\n",
    "        self.total_steps = torch.zeros(configs.num_env)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def rollout(self,batchsize,network : network):\n",
    "        self.clear()\n",
    "        self._observation,_ = self.env.reset()\n",
    "        for n in range(batchsize):\n",
    "            self._observation = torch.from_numpy(np.array(self._observation)).squeeze(-1).to(configs.device,torch.float32) / 255.\n",
    "            policy_output, value = network.forward(self._observation)\n",
    "            distribution = Categorical(policy_output)\n",
    "            action = distribution.sample()\n",
    "            prob = distribution.log_prob(action)\n",
    "            state,reward,done,_,_ = self.env.step(action.tolist())\n",
    "\n",
    "            for i in range(self.env.num_envs): \n",
    "                self.episode_reward[i] += reward[i] \n",
    "                self.total_steps[i] += 1\n",
    "                if done[i]:\n",
    "                    self.finished_reward.append(self.episode_reward[i])\n",
    "                    self.log_total_steps.append(self.total_steps[i])\n",
    "                    self.episode_reward[i] = 0\n",
    "                    self.total_steps[i] = 0\n",
    "\n",
    "            self.data.append(\n",
    "                [\n",
    "                    self._observation,\n",
    "                    torch.tensor(reward,dtype=torch.float32),\n",
    "                    value,\n",
    "                    prob,\n",
    "                    action,\n",
    "                    torch.from_numpy(done).to(torch.float32),\n",
    "                    distribution\n",
    "                ]\n",
    "            )\n",
    "            self._observation = state\n",
    "    \n",
    "        _,rewards,values,_,_,dones,_ = zip(*self.data)  \n",
    "        dones = torch.stack(dones).to(configs.device)\n",
    "        _rewards = torch.stack(rewards).to(configs.device) \n",
    "        values = torch.stack(values).squeeze(-1) \n",
    "        next_state = torch.from_numpy(np.array(self._observation)).squeeze(-1).to(configs.device,torch.float32) / 255.\n",
    "        _,next_value = network.forward(next_state)\n",
    "        _values = torch.cat([values,next_value.permute(-1,0)])\n",
    "        advantages = torch.zeros_like(_rewards,device=configs.device,dtype=torch.float32)\n",
    "        gae = 0.0\n",
    "        for n in reversed(range(len(_rewards))):\n",
    "            td = _rewards[n] + (self.gamma * _values[n+1] * (1-dones[n])) - _values[n]\n",
    "            gae = td + (self._lambda_ * self.gamma * gae * (1-dones[n])) \n",
    "            advantages[n] = gae\n",
    "\n",
    "        for data,item in zip(self.data,advantages): # append advantages to data\n",
    "            data.append(item)\n",
    "\n",
    "    def sample(self,number):\n",
    "        output = self.data[self.pointer:self.pointer+number]\n",
    "        self.pointer+=number\n",
    "        states,rewards,values,logProb,actions,done,old_logits,advantages = zip(*output)\n",
    "\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.stack(actions)\n",
    "        values = torch.stack(values).squeeze().view(-1)\n",
    "        logProb= torch.stack(logProb)\n",
    "        advantages = torch.stack(advantages).view(-1)\n",
    "\n",
    "        old_logits = [dis.probs for dis in old_logits]\n",
    "        old_logits = torch.stack(old_logits)\n",
    "        return (\n",
    "            states,actions,values,logProb,advantages,old_logits\n",
    "        )\n",
    "\n",
    "    def clear(self):\n",
    "        self.data = []\n",
    "        self.pointer = 0\n",
    "    \n",
    "    def traj_reward(self):\n",
    "        return self.finished_reward,self.log_total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-28T23:21:43.356309Z",
     "iopub.status.idle": "2025-06-28T23:21:43.356551Z",
     "shell.execute_reply": "2025-06-28T23:21:43.356453Z",
     "shell.execute_reply.started": "2025-06-28T23:21:43.356441Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.distributions.kl import kl_divergence\n",
    "\n",
    "class agent:\n",
    "    def __init__(self,):\n",
    "        self.env = make_env()\n",
    "        self.network = model\n",
    "        self.memory = Memory(self.env)\n",
    "        self.writter = SummaryWriter(\"./\")\n",
    "\n",
    "    def save(self,k):\n",
    "        checkpoint = {\n",
    "            \"model_state\" : self.network.state_dict(),\n",
    "            \"optim_state\" : self.network.module.optim.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint,f\"./mario{k}\")\n",
    "\n",
    "    def train(self,num_game,batchsize,minibatch,optim_steps):\n",
    "        for traj in range(num_game):\n",
    "            self.memory.rollout(batchsize,self.network)\n",
    "\n",
    "            for _ in range(batchsize//minibatch):\n",
    "                states,actions,old_values,old_log_prob,advantages,old_logits = self.memory.sample(minibatch) \n",
    "                vtarget = (advantages + old_values)\n",
    "                explained_variance = 1.0 - (torch.var(vtarget - old_values) / torch.var(vtarget + 1e-10))\n",
    "                cat_old_logits = Categorical(probs=old_logits)\n",
    "                flat_states = torch.flatten(states,0,1).to(configs.device) # - > [(minibatch * num_env), channel, image_shape]\n",
    "                \n",
    "                for _ in range(optim_steps): \n",
    "                    p_output,new_values = self.network.forward(flat_states)\n",
    "                    policy_output = p_output.view(minibatch,configs.num_env,7) # 7 = action space for a single env\n",
    "                    dist = Categorical(probs=policy_output)  \n",
    "                    kl = kl_divergence(cat_old_logits,dist).mean() \n",
    "                    new_log_prob = dist.log_prob(actions)\n",
    "                    ratio = torch.exp(new_log_prob - old_log_prob).view(-1) # the first ration should be = 1\n",
    "                    #norm_advantage  = (_advantages - _advantages.mean()) / (_advantages.std() + 1e-9) \n",
    "                    prox1 = ratio * advantages\n",
    "                    prox2 = torch.clamp(ratio ,1-configs.epsilon,1+configs.epsilon) * advantages\n",
    "\n",
    "                    loss_policy = -torch.mean(torch.min(prox1,prox2))  \n",
    "                    loss_critic = F.smooth_l1_loss(new_values.squeeze(),vtarget) * configs.critic_coeff\n",
    "                    entropy = dist.entropy().mean() * configs.entropy_coeff\n",
    "                    #_entropy = -(policy_output * torch.log(policy_output + 1e-10)).sum(dim=2)\n",
    "                    #entropy = _entropy.sum(dim=1).mean() * configs.entropy_coeff\n",
    "                    \n",
    "                    total_loss = loss_policy + loss_critic - entropy \n",
    "\n",
    "                    self.network.module.optim.zero_grad()\n",
    "                    total_loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
    "                    self.network.module.optim.step()\n",
    "                    \n",
    "            self.writter.add_scalar(\"Policy/entropy\",dist.entropy().mean(),traj)\n",
    "            self.writter.add_scalar(\"Policy/loss policy\",loss_policy,traj)\n",
    "            self.writter.add_scalar(\"Policy/KL\",kl,traj)\n",
    "            self.writter.add_scalar(\"Value/values\",new_values.detach().mean(),traj)\n",
    "            self.writter.add_scalar(\"Value/vtarget\",vtarget.mean(),traj)\n",
    "            self.writter.add_scalar(\"Value/value loss\",loss_critic,traj) \n",
    "            self.writter.add_scalar(\"Value/Explained variance\",explained_variance,traj)\n",
    "            self.writter.add_scalar(\"main/total loss\",total_loss,traj)\n",
    "            self.writter.add_scalar(\"main/epi rewards\",torch.tensor([self.memory.traj_reward()[0]]).mean(),traj) \n",
    "            self.writter.add_scalar(\"main/total steps\",torch.tensor([self.memory.traj_reward()[1]]).mean(),traj)\n",
    "\n",
    "            epireward = torch.tensor([self.memory.traj_reward()[0]]).mean().tolist()\n",
    "            print(\n",
    "                f\"{traj}/1k | REWA {epireward:.2f} | ENTR {entropy:.4f} | POLI {loss_policy:.2f} | CRIT {loss_critic:.2f} | TLoss {total_loss:.2f} | Val {new_values.detach().mean():.2f} | Vtarg {vtarget.mean():.2f} | KL {kl:.4f}\"\n",
    "                )\n",
    "            if traj % 10 == 0 :  \n",
    "                self.save(traj)\n",
    "    \n",
    "    @staticmethod\n",
    "    def run_training(run=False):\n",
    "        if run:\n",
    "            agnt = __class__()\n",
    "            agnt.train(num_game=configs.num_game,batchsize=configs.batchsize,minibatch=configs.minibatch,optim_steps=configs.optim_steps) \n",
    "            \n",
    "agent.run_training(True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": false,
     "modelId": 390353,
     "modelInstanceId": 369482,
     "sourceId": 455622,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 391029,
     "modelInstanceId": 370141,
     "sourceId": 456448,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 391180,
     "modelInstanceId": 370295,
     "sourceId": 456636,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 391230,
     "modelInstanceId": 370342,
     "sourceId": 456713,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 391336,
     "modelInstanceId": 370436,
     "sourceId": 456822,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 391789,
     "modelInstanceId": 370890,
     "sourceId": 457380,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
